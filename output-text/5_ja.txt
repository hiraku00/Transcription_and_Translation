LOMを使って複雑なアプリケーションを構築する場合、重要なステップの一つですが、時に厄介なのは、すべてのアプリケーションの出来をどのように評価するかということです。
それは、ある精度基準を満たしているのでしょうか？また、実装を変更する場合、別のLOMに入れ替えたり、チャンネルを取得するために影響を受けるデータベースや他のものを使用する戦略を変更したり、システムの他のパラメータを変更したりすることがあります。
このような場合、どのようにすれば良いのか、悪いのかが分かるのでしょうか？このビデオでは、ハリソンがLOMベースのアプリケーションの評価を考えるためのフレームワークと、それを支援するツールを紹介します。
このようなアプリケーションは、実に多くの異なるステップの連鎖とシーケンスです。
ですから正直なところ、最初にすべきことは、各ステップで何が行われ、何が出てくるのかを正確に理解することです。
そのため、いくつかのツールは、単にビジュアライザーやデバッガーとして考えることができます。
しかし、モデルがどのように動作しているのか、多くの異なるデータポイントでより全体的なイメージをつかむことは、しばしば本当に役に立ちます。
また、言語モデルそのものを使用して、他の言語モデルや他のチェーン、他のアプリケーションを評価するために、言語モデルそのものを変化させるという、とても素晴らしいアイデアもあります。
これについても詳しく説明します。
このように、たくさんの素晴らしいトピックがあります。
また、多くの開発で、プロンプトベースの開発、LOMを使ったアプリケーションの開発に移行することで、このワークフロー評価プロセス全体が見直されていることに気づきました。
このビデオには、たくさんのエキサイティングなコンセプトが詰まっています。
さっそく見てみましょう。
それでは、評価の準備をしましょう。
まず、最初に評価するチェーンやアプリケーションを用意する必要があります。
ここでは、前のレッスンで使用した文書質問応答チェーンを使用します。
そこで、必要なものをすべてインポートすることにします。
今まで使っていたのと同じデータを読み込みます。
1行でインデックスを作成します。
そして、言語モデル、チェーン・タイプ、リトリーバー、保護する冗長性を指定して、検索QAチェーンを作成することになります。
このアプリケーションを手に入れました。
最初にすべきことは、このアプリケーションを評価するためのデータポイントを見つけることです。
そのために、いくつかの異なる方法を紹介します。
まず、一番簡単なのは、自分たちで良い例だと思うデータポイントを思いつくことです。
そのためには、いくつかのデータを見て、質問例とグランドトゥルースとなる回答例を考え、それを後で評価するために使うことができます。
ここでいくつかのドキュメントを見てみると、その内部で何が起こっているのかがわかると思います。
これは最初のものです。
このプルオーバーのセットがありますね。
2枚目にはこれがありますね。
このジャケットがあるように、どれもディテールがたくさんあります。
そして、これらの詳細から、いくつかのクエリと答えのペアの例を作ることができます。
最初のクエリは、「コージーコンフォートプルオーバーセットにはサイドポケットがありますか」というシンプルなものです。
という質問です。
上を見ると、実際にサイドポケットがあることがわかります。
そして2つ目は、このジャケットはあるコレクション、ダウンテックコレクションのものであることがわかります。
では、このジャケットはどのコレクションのものでしょうか？と尋ねると、ダウンテックコレクションという答えが返ってきます。
このように、2つの例を作成しました。
しかし、これはそれほどうまくスケールしません。
それぞれの例に目を通し、何が起こっているのかを把握するのに少し時間がかかります。
そこで、これを自動化する方法はないでしょうか。
私たちが考える自動化できる素晴らしい方法の1つは、言語モデルそのものを使うことです。
そこで、まさにそれを実現するためのチェーンとチェーンがあります。
つまり、QA生成チェーンをインポートすることができるのです。
これはドキュメントを取り込み、それぞれのドキュメントから質問と回答のペアを作成するものです。
これは、言語モデルそのものを使って行われます。
そこで、チャットを開いたAI言語モデルを渡して、このチェーンを作成する必要があります。
そしてそこから、たくさんの例を作ることができます。
これは出力パーサーを結果に適用するためで、単一の文字列だけでなく、クエリと答えのペアを持つ辞書を取得したいからです。
クエリとアンサーを確認し、これが質問と回答であるドキュメントをチェックしましょう。
そして、この文書が質問と回答であることを確認してみましょう。
ここから重量を取得していることがわかります。
このように、質問と答えのペアを大量に生成しました。
全部自分で書かなくてもいいんです。
時間を節約して、もっとエキサイティングなことができるようになりました。
それでは、これらの例をすでに作成した例の中に追加してみましょう。
さて、このような例ができましたが、具体的にどのように評価すればいいのでしょうか。
まず最初に、チェーンを通してサンプルを実行し、その出力を見てみましょう。
ここでは、クエリを渡すと、答えが返ってきます。
しかし、これはチェーンの内部で実際に起こっていることを見るという意味では、少し限定的なものです。
言語モデルに入力される実際のプロンプトは何なのか？検索するドキュメントは何なのか？これが複数のステップを持つより複雑なチェーンであった場合、中間結果はどうなるのか？連鎖の中で何が間違っているのか、何が間違っている可能性があるのかを理解するためには、最終的な答えを見るだけでは不十分なことがよくあるのです。
そのために、chainにはchain debugという楽しい機能があります。
chain debugをtrueに設定して、上と同じ例を再実行すると、より多くの情報が出力されるようになるのがわかるでしょう。
何を出力しているのかを見てみると、まず検索QAチェーンに潜り込んでいることがわかります。
そして、次にドキュメントチェーンに潜り込んでいます。
つまり、前述のように、私たちはstuffメソッドを使っていたのです。
そして、LLMチェーンに入り、いくつかの異なる入力があります。
元の質問がそこにあるのがわかります。
そして今、このコンテキストを渡しています。
このコンテキストは、検索したさまざまな文書から作成されていることがわかります。
質問応答の場合、間違った結果が返されることがありますが、それは必ずしも言語モデルそのものに問題があるわけではありません。
実は、検索のステップで失敗しているのです。
そのため、質問の内容や文脈をよく観察することで、何が間違っているのかをデバッグすることができます。
さらにもう一段階踏み込んで、言語モデルチャットのオープンAIに何が入力されているかを正確に確認することができます。
ここでは、入力されたプロンプトをすべて見ることができます。
システムメッセージが表示されています。
使用されているプロンプトの説明もあります。
このプロンプトは質問応答チェーンが水面下で使用しているもので、実は今まで見ていなかったものです。
プロンプトがプリントアウトされ、ユーザーの質問に答えるために次の文脈を使用することがわかります。
答えがわからない場合は、ただ「わからない」と答えてください。
答えを作り出そうとしないでください。
そして、先ほど挿入したようなコンテキストの束が表示されます。
そして、私たちが質問した内容であるヒューマン・クエスチョンを見ることができます。
また、実際の戻り値の型についても、より多くの情報を見ることができます。
単なる文字列ではなく、トークンの使用状況、プロンプトトークン、完了トークン、合計トークン、モデル名など、さまざまな情報を得ることができます。
これは、チェーンや言語モデルの呼び出しで使用しているトークンを長期にわたって追跡し、トークンの総数を把握するのにとても便利です（これは総コストと非常に密接に対応しています）。
これは比較的単純なチェーンなので、最終的なレスポンスである「cozy comfort pullover set stripe, does have side pockets」がチェーンを通してバブルアップされ、ユーザーに返されることがわかります。
ここまでで、このチェーンに入力された1つのデータについて、何が起こっているのかを確認し、デバッグする方法を説明しました。
しかし、私たちが作成したすべての例についてはどうでしょうか？どうやって評価するのでしょうか？作成するときと同様、手動で評価する方法もあります。
すべての例に対してチェーンを実行し、出力を見て、何が起こっているのか、正しいのか、間違っているのか、部分的に正しいのかを把握しようとするのですが、例を作るのと同じように、時間が経つにつれて少し面倒になってきます。
そこで、私たちのお気に入りの解決策に戻りましょう。
言語モデルに頼めばいいのでしょうか？まず、すべての例文について予測値を作成する必要があります。
その前に、デバッグモードをオフにして、画面にすべてを出力しないようにします。
そして、すべての異なる例について予測を作成するつもりです。
全部で7つの例があったと思います。
この連鎖を7回ループして、それぞれの予測値を取得することにします。
さて、このような例題ができたので、次はその評価について考えてみましょう。
そこで、QA question answering, Eval chainをインポートすることにします。
このチェーンは、言語モデルで作成します。
なぜなら、ここでも言語モデルを使用して評価を行うからです。
そして、このチェーンに対してevaluateを呼び出すことにします。
例文と予測値を渡すと、評価された出力が返ってきます。
そして、それぞれの例で何が起こっているのかを確認するために、ループを回してみます。
問題をプリントアウトします。
これは言語モデルによって生成されたものです。
本当の答えを出力してみましょう。
これも言語モデルによって生成されたもので、文書全体を前にしています。
そのため、真実の答えを生成することができました。
予測された答えをプリントアウトしてみましょう。
これは、QAチェインを行う際に言語モデルによって生成されたもので、埋め込みとベクトルデータベースを使って検索を行い、それを言語モデルに渡して、予測された答えを推測しようとするものです。
そして、それをプリントアウトして成績も出します。
これも言語モデルによって生成され、Eval chainに何が起こっているのか、正しいのか間違っているのかを採点するよう依頼します。
このように、すべての例をループしてプリントアウトすると、それぞれの例について詳細な情報を見ることができます。
そして、このように、すべてが正しくなっています。
これは比較的単純な検索問題です。
ですから、これは心強いことです。
では、最初の例を見てみましょう。
この問題は、Kozy Comfortのプルオーバーセットにはサイドポケットがあるかというものです。
本当の答えは「YES」です（私たちはこれを作成しました）。
言語モデルが予測した答えは、「Kozy Comfort pull-over-set stripe does have side pockets（コージー・コンフォート プルオーバーセット ストライプにはサイドポケットがある）」でした。
ということで、これが正しい答えであることが理解できます。
そして実際に、言語モデルも同様に、正解と判定しています。
しかし、そもそもなぜ言語モデルを使う必要があるのか、考えてみましょう。
この2つの文字列は、実は似て非なるものです。
全く違うものなのです。
一方は本当に短く、一方は本当に長い。
この文字列には、「はい」はどこにも出てきませんね。
だから、文字列の照合や完全照合、あるいは「Xはここにある」というような照合をしようとしても、どうしたらいいのかわからない。
これらは同じものではないのです。
これは、言語モデルを使って評価することの重要性を示しています。
回答は任意の文字列です。
最適な答えとなる真実の文字列は1つだけではありません。
さまざまなバリエーションがあります。
そして、それらが同じ意味を持っている限り、似ていると評価されるはずです。
そのために、言語モデルは、単に完全なマッチングを行うだけでなく、その手助けをするのです。
この文字列の比較の難しさが、そもそも言語モデルの評価を難しくしているのです。
私たちは言語モデルを、テキストを生成するような自由度の高いタスクに使っています。
というのも、最近まで言語モデルの性能が十分でなかったため、このようなことは行われていませんでした。
そのため、これまで存在していた評価指標の多くは、十分なものではありませんでした。
そのため、私たちは新しい評価指標を考案し、そのための新しい方法論を考案しなければなりません。
そして、その中で最も興味深く、最も人気があるのが、言語モデルを使って評価を行う方法です。
これで評価のレッスンは終わりです。
しかし、最後にもう一つお見せしたいのは、チェーン評価プラットフォームです。
これは、今ノートブックでやったことをすべて、永続化してUIで表示する方法です。
では、実際に見てみましょう。
ここでは、セッションがあることがわかります。
ディープラーニングAIと名付けました。
そして、ノートブックで実行したすべての実行を実際に永続化したことがわかります。
このように、入力と出力を高いレベルで追跡するのに良い方法です。
しかし、その下で何が起こっているのかを知るには、とても良い方法です。
これは、デバッグモードをオンにしたときにノートブックに出力されたのと同じ情報です。
ただ、UIで少しきれいに可視化されているだけです。
このように、各ステップでチェーンの入力と出力を見ることができます。
さらに、チェーンの下をクリックすると、実際に何が渡されるのか、より詳細な情報を見ることができます。
つまり、一番下まで降りていくと、チャットモデルに何が正確に渡されているのかがわかるのです。
システムメッセージはここにあります。
人間の質問がここにあります。
チャットモデルからの応答がここにあり、いくつかの出力メタデータがあります。
もうひとつ、データセットに例を追加する機能が追加されました。
冒頭で例のデータセットを作成したとき、一部は手作業で、一部は言語モデルで作成したことを思い出してください。
この小さなボタンをクリックすると、入力クエリと出力結果が表示され、データセットに追加することができます。
こうして、データセットを作ることができます。
これをディープラーニングと呼びます。
そして、このデータセットに例を追加していくことができます。
もう一度、レッスンの最初に取り組んだことに戻りますが、評価を行うためにはデータセットを作成する必要があります。
これは、バックグラウンドで実行し、時間の経過とともに例のデータセットを追加し、評価に使用できる例を構築し始め、評価のフライホイールが回り始めるという、実に良い方法なのです。
