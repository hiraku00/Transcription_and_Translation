 In lesson one, we'll be covering models, prompts, and parsers.
So models refers to the language models underpinning a lot of it.
Promps refers to the style of creating inputs to pass into the models.
And then parsers is on the opposite end.
It involves taking the output of these models and parsing it into a more structured format so that you can do things downstream with it.
So when you build applications using LM, they'll often be reusable models.
We repeatedly prompt a model, parsers outputs, and so Lanching gives an easy set of attractions to do this type of operation.
So with that, let's jump in and take a look at models, prompts, and parsers.
So to get started, here's a little bit of static code.
I'm going to import OS, import OpenAI, and load my OpenAI secret key.
The OpenAI libraries are already installed in my Jupyter notebook environment.
They're running this locally, and you don't have OpenAI installed yet.
You might need to run that.
Bank PIP install OpenAI, but I'm not going to do that here.
And then here's a helper function.
This is actually very similar to the helper function that you might have seen in the chat GPT-promangering for developer scores that are off it together with OpenAI's Ether-for-Fit.
And so with this helper function, you can say get the completion on what is 1 plus 1, and this will call chat GPT, or technically the model GPT-3.5 terrible to give you an answer back like this.
Now, to motivate the Lanchane abstractions for model prompts and pauses, let's say you get an email from a customer in a language other than English.
In order to make sure this is accessible, the other language I'm going to use is the English pirate language, where the customers are, I'd be fuming that we blend it through off and splat in my kitchen walls with smoothie.
And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen.
I need your help right now, matey.
And so what we will do is ask this LOM to translate the text to American English in a calm and respectful tone.
So I'm going to set style to American English in a calm and respectful tone.
And so in order to actually accomplish this, if you've seen a little bit of prompting before, I'm going to specify the prompt using an F string with instructions, translate the text that is delimited by triple-bactics into style that is style and plug in these two styles.
And so this generates a prompt that says translate the text and so on.
I encourage you to pause the video and render code and also try modifying the prompt to see if you can get a different output.
You can then prompt the large language model to get a response.
Let's see what the response is.
So to translate to the English pirate's message into this very polite, I'm really frustrated that my blender lit flew off and made a mess of my kitchen walls with smoothie and so on.
I could really use your help right now, my friend.
That sounds very nice.
So if you have different customers writing reviews in different languages, not just English pirate, but French, German, Japanese and so on, you can imagine having to generate a whole sequence of prompts to generate such translations.
Let's look at how we can do this in a more convenient way using land chain.
I'm going to import chat open AI.
This is land chains abstraction for the chat GPT API endpoint.
And so if I then set chat goes chat open AI and look like chat is, it creates this object as follows that uses the chat GPT model, which is also called GPT 3.5 turbo.
When I'm building applications, one thing I will often do is set to temperature parameter to be equal to zero.
So the default temperature is 0.7.
But let me actually redo that with temperature equals 0.0.
And now the temperature is set to zero to make us output a little bit less random.
And now let me define the template string as follows.
Translate the text, deliberate it by triple-vatics into style, the style, and then use the text.
And to repeatedly reuse this template, let's import land chains chat prompt template.
And then let me create a prompt template using that template string that we just wrote above.
From the prompt template, you can actually extract the original prompt.
And it realizes that this prompt has two input variables, the style and the text, which were shown here with the curly braces.
And here is the original template as well that we had specified.
In fact, with the print this out, it realizes it has two input variables, style and text.
Now let's specify the style.
This is a style that I want the customer message to be translated to.
So I'm going to call this customer style.
And here's my same customer email as before.
And now, if I create customer messages, this would generate the prompt.
And we'll pass this a large language model in a minute, together with spawn.
So if you want to look at the types, the customer message is actually a list.
And if you look at the first element of the list, this is more or less that prompt that you would expect this to be creating.
Lastly, this process prompt to the OM, so I'm going to call chat, which we had said earlier as a reference to the OpenAI chat GPD endpoint.
And if we print out the customer response content, then it gives you back this text translated from English Pirate to polite American English.
And of course, you can imagine other use cases where the customer emails are in other languages.
And this tool can be used to translate the messages from English speaking to understand and reply to.
I encourage you to pause the video and run the code and also try modifying the prompt to see if you can get a different output.
Now, let's hope our customer service agent reply to the customer in their original language.
So let's say English speaking customer service agent writes this is a hey there customer warranty does not cover clean spends this for a kitchen because it's your fault that you miss user blender by figuring it on the lid tough luck.
See, not a very polite message, but let's say this is one of the customer service agent once.
We are going to specify that the service message is going to be translated to this pirate style.
So we wanted to be in a polite tone that speaks in English pirate.
And because we previously created that prompt template, the cool thing is we can now reuse that prompt template and specify that the office now we want is this service style pirate.
And the text is this service reply.
And if we do that, that's the prompt.
And if we prompt on charge GPT, this is response it gives us back.
Oh, I don't know, matey, I must kind of inform you that the warranty be not covering expenses or cleaning your galley.
And so on, I tough luck, Pharaoh me hearty.
So you might be wondering why are we using prompt templates instead of just an F string.
The answer is that as you build sophisticated applications, prompts can be quite long and detailed.
And so prompt templates are a useful abstraction to help you reuse good prompts when you can.
This is an example of a relatively long prompt to grade a student submission for an online learning application.
And a prompt like this can be quite long in which you can ask the LM to first solve the problem and then have the output in a certain format and output in a certain format.
And wrapping this in a lang chain prompt makes it easier to reuse a prompt like this.
Also, you see later that lang chain provides prompts for some common operations such as summarization or question answering or connecting to SQL databases or connecting to different APIs.
And so by using some of lang chains built in prompts, you can quickly get an application working without needing to engineer your own prompts.
One other aspect of lang chains prompt libraries is that it also supports output pausing, which we'll get to in a minute.
But when you're building a complex application using an LM, you often instruct the LM to generate its output in a certain format such as using specific keywords.
This example on the left illustrates using an LM to carry out some of the called chain of thoughts reasoning using a framework called the react framework.
But don't worry about the technical details, but the keys of that is that the thought is what the LM is thinking because by giving an LM space to think it can often get to more accurate conclusions.
Then action as a keyword to carry the specific action and then observation to show what it learned from that action and so on.
And if you have a prompt that instructs the LM to use these specific keywords, thought, action, and observation, then this prompt can be coupled with a parser to extract out the text that has been tagged with these specific keywords.
And so that together gives a very nice abstraction to specify the input to an LM and then also have a parser correctly interpret the output that the LM gives.
And so with that, let's return to see an example of an output parser using lang chain.
In this example, let's take a look at how you can have an LM output JSON and use lang chain to parse that output.
And the running example that I'll use will be to extract information from a product review and format that output in a JSON format.
So here's an example of how you would like the output to be formatted.
Technically, this is a Python dictionary where whether or not the product has a gift, masterfalls, the number of days took to deliver was five and the price value was pretty affordable.
So if this is one example of a desired output, here is an example of customer review as well as a template to try to get to that JSON output.
So here's a customer review.
It says, this lead blower is pretty amazing.
It has four settings, candle blower, gender breeze, windy city, and tornado.
It arrived in two days just in time from a wise anniversary present.
I think my wife liked the so much of speech list so far been the only one using it and so on.
And here's a review template for the following text extract of all the information specified was this a gift.
So in this case, it would be yes because this is a gift.
And also delivery days, how long did it take to deliver? It looks like in this case, in the right in two days.
And what's the price value? You know, slightly more expensive on a lead blower and so on.
So the review template asked the OM to take us input a customer review and extract these three fields and then format the output as JSON with the following keys.
All right.
So here's how you can wrap this in line chain.
Let's import the chat prompt template.
We actually imported this already earlier.
So technical use lines redundant but I'll just import the game and then have the prompt template on created from the review template up on top.
And so here's the prompt template.
And now sum of the early usage of a prompt template, let's create the messages to pass to the opening eye endpoint, create the opening eye endpoint, call that endpoint and then let's print out the response.
I encourage you to pause the video and run the code.
And there are this.
It says give us through delivery days is two and the price value also looks pretty accurate.
But note that if we check the type of the response, this is actually a string.
So it looks like JSON and looks like as key value pairs but it's actually not to dictionary.
This is just one long string.
So what I really like to do is go to the response content and get the value from the give key, which should be true.
But I've run this, this should generate an error because well, this is actually a string.
This is not a Python dictionary.
So let's see how we would use Lankin's parser in order to do this.
I'm going to import response schema and structured output parser from Lankin.
And I'm going to tell it what I wanted to pause by specifying these response schema.
So the give schema is named gift and here's the description was the item purchase we give for someone else.
On the true of yes, false if not to unknown and so on.
So have a give schema delivery day schema price value schema and then let's put all three of them into a list as follows.
Now that I've specified the schema for these, Lankin can actually give you the prompt itself by having the output parser tell you what instructions it wants you to send to the LM server where to print formats instructions.
She has a pretty precise set of instructions for the LM that will cause the degenerate outputs that the output parser can process.
So here's a new review template and the review template includes the format instructions that Lankin generated and so can create a prompt from the review template to and then create the messages that will pass to the open AI endpoint.
If you want you can take a look at the actual prompt which gives the instructions to extract the fused gift delivery days price value.
Here's the text and then here are the formatting instructions.
Finally, if we call the open AI endpoint, let's take a look at what response we got.
It is now this and now if we use the output parser that we created earlier, you can then pause this into an output dictionary.
We should have I print looks like this and notice that this is um of type dictionary, not a string which is why I can now extract the value associated with the key gift and get true or the value associated with delivery days and get to or you can also extract the value associated with price value.
So this is a nifty way to take your LM output and parse it into a Python dictionary to make the output easier to use in downstream processing.
I encourage you to pause the video and run the code and so that's it for models, prompts and parsers with these tools.
Hopefully you will be able to reuse your own prompt templates easily, share prompt templates with others that you're collaborating with, even use line chains built in prompt templates which as you just saw can often be coupled with an output parser so that the input prompt to output in a specific format and in a parser pauses that output to store the data in a Python dictionary or some of the data that makes it easy for downstream processing.
I hope you find this useful in many of your applications and with that let's go into the next video where we'll see how LAN came can help you build better chatbots, we have an RM had more effective chats by better managing what it remembers from the conversation you've had so far.