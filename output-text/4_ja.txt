LAMを使って構築される最も一般的な複雑なアプリケーションの1つは、ドキュメントの上やドキュメントに関する質問に答えることができるシステムです。
PDFファイルやウェブページ、あるいは企業のイントラネットの内部文書集から抽出したテキストがあれば、LAMを使って文書の内容に関する質問に答え、ユーザーがより深く理解し、必要な情報にアクセスできるようにすることができるのではないでしょうか。
これは本当に強力なことです。
なぜなら、言語モデルを、元々トレーニングされていないデータと組み合わせるようになるからです。
そのため、ユースケースに対してより柔軟で適応性のあるものにすることができます。
また、言語モデル、プロンプト、出力パーサーを越えて、埋め込みモデルやベクトルストアなど、LAMの主要なコンポーネントを導入し始めるので、とても楽しみです。
アンドリューが言ったように、これは私たちが持っているより人気のあるチェーンの1つなので、期待してほしいです。
実は、エンベッディングはベクトルストアなんです。
現代の最も強力なテクニックのひとつなので、まだ見たことがない方は、ぜひとも学んでみてください。
それでは、さっそく飛び込んでみましょう。
さあ、やってみましょう。
まず、いつものように環境変数をインポートします。
次に、このチェーンを構築する際に役立つものをいくつかインポートします。
ここでは、検索QAチェーンをインポートすることにします。
これは、いくつかの文書に対して検索を行います。
私たちのお気に入りのチャット、オープンAI、言語モデルをインポートします。
ドキュメントローダーをインポートします。
これは、言語モデルと組み合わせる独自のデータをロードするために使用されます。
この場合、CSVになります。
そこで、CSVローダーをインポートします。
最後に、ベクターストアをインポートします。
ベクターストアにはさまざまな種類がありますが、具体的にどのようなものかについては後ほど説明します。
しかし、ここではDocker Aのインメモリ検索ベクターストアから始めることにします。
これはインメモリ・ベクターストアなので、本当に素晴らしいです。
外部のデータベースに接続する必要がありません。
だから、本当に簡単に始めることができるのです。
また、Jupyterノートブックで情報を表示するための2つの一般的なユーティリティをインポートして表示とマークダウンを行う予定です。
言語モデルとの組み合わせに使用するアウトドアウェアのCSVを用意しました。
ここでは、このファイルへのパスを指定して、ローダーであるCSVローダーを初期化することにします。
次に、インデックスをインポートします。
ベクトルストアのインデックス作成ツールです。
これを使えば、本当に簡単にベクターストアを作成することができます。
下にあるように、これを作成するのは数行のコードだけです。
これを作るには、2つのことを指定する必要があります。
まず、ベクターストアクラスを指定します。
前述したように、このベクターストアは特に簡単に始めることができるので、これを使うことにします。
作成後は、ドキュメント・ローダーのリストを受け取るfrom loadersを呼び出すことになります。
私たちが本当に気になるローダーは1つだけです。
だから、ここではそれを渡しているのです。
これでローダーが作成され、それについて質問することができるようになりました。
以下では、ボンネットの中で何が起こっているのかを説明します。
ですから、今はそのことは気にしないでください。
ここでは、まずクエリを作成します。
そして、インデックスクエリを使用してレスポンスを作成し、このクエリを渡します。
繰り返しますが、この後、ボンネットの中で何が起こっているかは後ほど説明します。
とりあえず、応答があるのを待ちます。
応答が終わったら、何が返されたかを見てみましょう。
Markdownのテーブルに、日焼け止めを施したシャツの名前と説明文が格納されています。
また、言語モデルが提供してくれた素敵な要約もあります。
さて、ここまで文書に質問応答を行う方法について説明してきましたが、フードの下では一体何が行われているのでしょうか？まず、一般的なアイデアについて考えてみましょう。
私たちは言語モデルを使い、それを多くの文書と組み合わせたいと考えています。
しかし、重要な問題があります。
言語モデルは、一度に数千語しか検査できません。
では、本当に大きな文書があった場合、そこにあるすべてのものについての質問に言語モデルが答えられるようにするにはどうしたらいいでしょうか？そこで登場するのが、エンベッディングとベクトルストアです。
まず、エンベッディングについて説明しましょう。
エンベッディングは、テキストの断片を数値で表現します。
この数値表現には、そのテキストが持つ意味的な意味が含まれています。
似たような内容のテキスト片は、似たようなベクトルを持つことになります。
これにより、ベクトル空間内でテキスト片を比較することができます。
下の例では、3つの文章があることがわかります。
最初の2つはペットに関するものです。
そして、3つ目は車に関するものです。
数値空間での表現を見ると、ペットに関する文章に対応するテキスト片の2つのベクトルを比較すると、非常に似ていることがわかります。
では、車についての文章に対応するものと比較すると、両者はまったく似ていない。
このように、どのテキストが互いに似ているかを簡単に把握することができます。
これは、質問に答えるために言語モデルに渡す際に、どのテキストを含めるかを考える際に非常に便利です。
次に取り上げるコンポーネントは、ベクトルデータベースです。
ベクトルデータベースは、前のステップで作成したベクトル表現を保存するためのものです。
このベクトルデータベースを作成する方法は、入力される文書から得られるテキストのチャンクを入力することです。
大きな文書が送られてきたら、まずそれを小さな塊に分割していきます。
こうすることで、元の文書よりも小さなテキストの断片を作ることができます。
これは、言語モデルに文書全体を渡すことができない場合に役立ちます。
つまり、小さな塊を作ることで、最も関連性の高いものだけを言語モデルに渡せるようにしたいのです。
そして、それぞれのチャンクの埋め込みを作成し、それをベクターデータベースに保存します。
これが、インデックスを作成するときの流れです。
このインデックスを手に入れたら、実行時にそれを使って、入ってきたクエリに最も関連性の高いテキストの断片を探し出すことができます。
クエリが来たら、まずそのクエリのエンベッディングを作成します。
そして、ベクトルデータベースにあるすべてのベクトルと比較し、最も似ているものを選びます。
そして、それらを言語モデルに渡すことで、最終的な答えを返すことができるのです。
このように、わずか数行のコードで、この連鎖を作ることができました。
これは、すぐに始めるには最適です。
しかし、今度はもう少しステップ・バイ・ステップで、ボンネットの下で何が起こっているのかを正確に理解しましょう。
最初のステップは上記と同様です。
ドキュメント・ローダーを作成し、質問応答を行いたい製品のすべての説明を含むCSVから読み込みます。
そして、このドキュメント・ローダーからドキュメントを読み込むことができます。
個々のドキュメントを見ると、各ドキュメントがCSVの中の1つの製品に対応していることがわかります。
前回、チャンクを作るという話をしました。
これらのドキュメントはすでに非常に小さいので、実はここでチャンク化を行う必要はありません。
だから、直接エンベッディングを作ればいいのです。
エンベッディングを作るには、OpenAIのエンベッディングクラスを使うことにします。
これをインポートして、ここで初期化します。
このembeddingsが何をするのか見たい場合は、実際に特定のテキストを埋め込むとどうなるかを見てみましょう。
embeddingのオブジェクトのembed queryメソッドを使って、特定のテキスト片のembeddingsを作成してみましょう。
この場合、high, my name is Harrisonという文章です。
この埋め込みを見てみると、1000以上の異なる要素があることがわかります。
これらの要素は、それぞれ異なる数値です。
これを組み合わせることで、このテキストの全体的な数値表現ができあがります。
先ほど読み込んだすべてのテキストの埋め込みを作成し、さらにそれをベクターストアに格納したい。
これを実現するには、ベクトルストアのfrom documentsメソッドを使います。
このメソッドは、ドキュメントのリストと埋め込みオブジェクトを受け取り、全体のベクターストアを作成します。
このベクトルストアを用いて、入力されたクエリに類似したテキストの断片を見つけることができるようになります。
では、クエリを見てみましょう。
日焼け止めの入ったシャツを提案してください。
ベクトルストアの類似検索メソッドを使ってクエリを渡すと、ドキュメントのリストが返ってきます。
4つのドキュメントが返ってくることがわかりますが、最初のドキュメントを見てみると、確かに日除けに関するシャツであることがわかります。
では、これを利用して、自分たちの文書に対して質問応答を行うにはどうすればいいのでしょうか。
まず、このベクトルストアからレトリーバを作る必要があります。
レトリーバとは、クエリを受け取って文書を返すメソッドであれば、何でも使える汎用的なインターフェースです。
ベクターストアやエンベッディングは、そのような手法の一つですが、様々な手法があり、高度でないものもあれば高度なものもあります。
次に、テキストを生成して自然言語の応答を返したいので、言語モデルをインポートし、チャットオープンAIを使用することにします。
これを手作業で行う場合、どうするかというと、ドキュメントを組み合わせて1つのテキストにすることになります。
このように、ドキュメントのページ内容をすべて変数に結合します。
そして、この変数、あるいは質問のバリエーション、たとえば日焼け止めのついたシャツをすべてリストアップしてくださいとか、マークダウンした表などを渡して、それぞれを言語モデルにまとめます。
そして、ここで回答をプリントアウトすると、要求通りの表が返ってきていることがわかります。
これらのステップはすべて、ラングチェーンチェーンでカプセル化することができます。
ここでは、検索QHチェーンを作成します。
これは、検索を行い、検索された文書に対して質問応答を行うものです。
このようなチェーンを作成するために、いくつかの異なるものを渡すことにします。
まず、言語モデルを渡します。
これは、最後にテキストを生成するために使われます。
次に、チェーンの種類を指定します。
ここではstuffを使うことにします。
これは、すべての文書をコンテキストに詰め込み、言語モデルを1回呼び出すだけなので、最もシンプルな方法です。
質問応答には他にもいくつかの方法があり、最後に触れるかもしれませんが、ここでは詳しく見ていきません。
3つ目は、レトリーバを渡すことです。
上で作成したretrieverは、ドキュメントをフェッチするためのインターフェースに過ぎません。
これを使ってドキュメントをフェッチし、言語モデルに渡すことになります。
そして最後に、verbose equalsをtrueに設定します。
これでクエリーを作成し、スクエアでチェーンを実行することができるようになりました。
レスポンスが返ってきたら、display and mark downユーティリティを使って再び表示することができます。
ここでビデオを一時停止して、さまざまなクエリで試してみることができます。
というわけで、これが詳細なやり方です。
しかし、上に挙げた1行だけでも、かなり簡単にできることを忘れないでください。
つまり、この2つのことは同じ結果に等しいのです。
これがリンクチェインの面白いところです。
1行でできることもあれば、個々のものを見て、より詳細な5つのものに分解することもできるのです。
5つのより詳細なものは、具体的に何が起こっているのかをより具体的に設定することができますが、1行は簡単に始めることができます。
ですから、どのように参照しながら進めるかはあなた次第です。
また、インデックスを作成する際に、カスタマイズすることもできます。
手作業で作成したときは、埋め込みを指定しましたが、ここでも埋め込みを指定できます。
これにより、エンベッディング自体の作成方法を柔軟に変更することができます。
また、このベクターストアを別のタイプのベクターストアに交換することもできます。
このように、手書きで作成したときと同じレベルのカスタマイズが、インデックスを作成するときにも可能なのです。
このノートブックではstuffメソッドを使用しています。
stuffメソッドはとてもシンプルで良いですね。
一つのプロンプトに全部入れて、それを言語モデルに送って、一つのレスポンスが返ってくるだけなんです。
だから、何が起こっているのかを理解するのがとても簡単なんです。
非常に安価で、うまく機能します。
しかし、いつもうまくいくとは限りません。
ノートブックの文書を取得したとき、4つの文書しか返ってきませんでしたが、それは比較的小さなものでした。
では、同じような質問に対する回答を、たくさんの種類の塊に対して行いたい場合はどうすればいいのでしょうか？その場合、いくつかの異なる方法を使うことができます。
1つ目は、マップリダクションです。
これは基本的にすべてのチャンクを受け取り、質問と一緒に言語モデルに渡し、レスポンスを返し、さらに別の言語モデルを呼び出して個々のレスポンスをすべて最終的な答えにまとめます。
これは、いくつもの文書に対して操作できるため、非常に強力です。
また、個々の質問と並行して行うことができるので、本当に強力です。
しかし、その分、呼び出し回数が多くなります。
また、すべての文書を独立したものとして扱いますが、これは必ずしも望ましいこととは限りません。
Refineもまた、多くのドキュメントをループするために使用される方法です。
しかし、これは実際には反復的に行われます。
前の文書に書かれていた答えをもとに、その答えを作り上げていくのです。
ですから、情報を組み合わせ、時間をかけて答えを作り上げていくのに適しています。
一般に、より長い答えが得られるでしょう。
また、呼び出しが独立したものでないため、速度はそれほど速くありません。
以前の通話結果に依存するためです。
つまり、基本的にはmap reduceと同じように多くの呼び出しが必要で、時間がかかることが多いのです。
Map re-rankは非常に興味深いもので、もう少し実験的なものですが、文書ごとに言語モデルを1回呼び出すだけで、スコアも返してもらいます。
そして、最も高いスコアを選択するのです。
この場合、スコアがどうなるかは言語モデルに依存します。
そのため、「この文書に関連性があれば高得点になるはずだ」と言語モデルに指示し、指示を絞り込む必要があることもあります。
Map Redと同様、すべての呼び出しが独立しているので、バッチ処理で比較的高速に処理できます。
しかし、やはり言語モデルの呼び出しを何度も行うことになります。
だから、少し高くつくでしょう。
この中で一番多いのはstuffメソッドで、ノートブックで使っていたのは1つのドキュメントにまとめる方法です。
次に多いのがmap reduceメソッドで、これはこれらのチャンクを受け取って言語モデルに送るものです。
これらのメソッド、stuff、map reduce、refine、re-rankは、質問応答だけでなく、他のチェーンにも使用することができます。
例えば、map reduceのチェーンで本当によく使われるのは、長い文書があり、その中の情報の断片を再帰的に要約するような要約のケースです。
文書に対する質問応答は以上です。
お気づきかもしれませんが、ここで紹介したさまざまなチェーンでは、多くのことが行われています。
次のセクションでは、これらの連鎖の中で何が起こっているのかをよりよく理解する方法について説明します。
