CHAPAのようなアプリケーションを開発する場合、モデルと会話することが問題になります。
このセクションでは、記憶について説明します。
基本的には、会話の前の部分をどのように記憶し、それを言語モデルに送り込むことで、相手と対話する際に会話の流れを作ることができるのか、ということです。
そうですね。
ランチェーンオフィスでは、この記憶を管理するための洗練されたオプションを複数用意しています。
では、さっそく見てみましょう。
まず、OpenAIのAPIキーをインポートして、必要なツールをいくつかインポートしてみましょう。
ここでは、Lanchainを使ってチャットやチャットボットの会話を管理することを、記憶の動機付けの例として使ってみましょう。
そのために、OMをOpenAIのチャットインターフェースとしてTempshikul0に設定し、メモリを会話バッファのメモリとして使用します。
そして、会話の連鎖を構築していきます。
また、この講座の後半で、ハリソンがチェーンやランチェーンとは何かについて、もっと深く掘り下げて説明します。
だから、この構文の詳細については、ヘルプを見ながらあまり気にしないでください。
そして、もし私が会話を始めたら、会話ドットは予測し、私の名前はアンドリューですと入力を与えます。
どんなことが書いてあるか見てみましょう。
こんにちは、そしてはじめまして、でしょうか？といった具合です。
そして、「1＋1は何ですか」と聞いてみましょう。
1＋1は2です、で、また聞いてみる。
あのね、私の名前は何ですか？私の名前は、さっきの話だとアンドリューです。
そこはもっとサカナクションの痕跡があるんですよ、よくわからないけど。
それで、もし必要なら、この言語変数をtrueに変えて、Lanchainが実際に何をしているのか見てみることができます。
Andrewとpredict harmonyを実行すると、これはLanchainが生成するプロンプトです。
これは、「以下は、人間とAIのトップとしての友好的な会話です」などと書かれています。
つまり、これはLanchainが生成するプロンプトで、システムに役立つフレンドリーな会話をさせるためのものです。
そして、その会話を保存する必要があり、その応答がこちらです。
そして、これを会話の2番目と3番目の部分で実行すると、以下のようにプロンプトが保存されます。
そして、私が「私の名前は何ですか」と口にするまでに、次のようなプロンプトが表示されることに注目してください。
これが3回目のターンです。
これは私の3回目の入力です。
それは、現在の会話を次のように遅くしています。
アンドリューは何人ですか？1プラス1って何ですか？といった具合に。
そうして、この記憶、あるいは会話の履歴がどんどん長くなっていくのです。
実は、上の方で、記憶を保存するためにメモリ変数を使っていました。
ですから、memory dot bufferと表示すれば、ここまでの会話が記憶されていることになります。
このメモリドットロードのメモリ変数をプリントすることもできます。
ここの中括弧は、実は空の辞書です。
もっと高度な入力で使える機能もあるのですが、この短い通話ではお話しません。
ですから、なぜここが空っぽの中括弧なのかは気にしないでください。
ただ、これはランチェインがこれまでの会話の記憶の中で覚えているものです。
AIが言ったこと、あるいは人間が言ったこと、すべてです。
ぜひ動画を一時停止して、コードを実行してみてください。
つまり、Lanchainが会話を保存する方法は、この会話バッファーのメモリです。
会話バッファメモリは、いくつかの入力と出力を指定するために使用されていますが、このように新しいものを追加することができます。
このように、メモリに新しいものを追加していくのです。
明示的に行いたい場合は、memory dot save contextが、ハイ、どうしたんですか？これが一番エキサイティングな会話でないことは分かっていますが、短い例を用意したかったのです。
これで、メモリの状態はどうなっているかというと、こうなっています。
そしてもう一度、実際にメモリの変数を表示させてみましょう。
もし、メモリにデータを追加したい場合は、コンテキストを追加保存し続けることができます。
だから、会話は続くし、ただぶら下がるだけで、あまり意味がない、クールだ。
そして、メモリをプリントアウトすると、もう何も入っていません。
つまり、チャットの会話に大規模な言語モデルを使う場合、実は大規模な言語モデル自体がステートレスなのです。
言語モデル自体が、これまでの会話を覚えているわけではありません。
そして、各トランザクション、APIエンドポイントへの各コールは独立しています。
そして、チャットのボスは純粋にメモリにのみ存在します。
なぜなら、通常、LMのコンテキストとして、これまでに行われた完全な会話を提供する迅速なコードが存在するからです。
そのため、メモリはこれまでの発言のターンを明示的にスロー再生することができる。
こんにちは、私の名前はアンドリューです、その他、私やあなたにとって興味深いことがありました、など。
そして、この記憶の保存は、LMへの入力または追加のコンテキストとして使用され、LMは、ちょうどミックス会話のターンを持っているように出力を生成することができます。
前に何が言われたかを知ることができる。
会話が長くなると、必要なメモリの量も本当に長くなり、トークンを大量にLMに送るコストも高くなります（通常、このプロセスではトークンの数に応じて課金されます）。
そこで、このチェーンでは、会話を保存・蓄積するための便利なメモリーを何種類か用意しています。
これまでは、会話バッファーのメモリを見てきました。
別の種類のメモリを見てみましょう。
今回取り上げるのは、会話バッファのウィンドウ・メモリだけを保持するものです。
K equals 1で会話バッファ・ウィンドウ・メモリにメモリが送られました。
Kイコール1という変数は、会話のやりとりを1つだけ覚えておきたいということを指定している。
つまり、私から回したものと、チャットボットから回したものを1つずつです。
これで、コンテキストを保存するようにすれば、私はぶら下がるだけで大したことはありませんでした。
memory.loverablesを見ると、直近の発言しか記憶していません。
このように落ちていることに注目してください。
私は起きていました。
これは、人間が「大したことない」と言っただけで、AIは「かっこいい」と言ったのです。
つまり、Kが1になったからです。
この機能は、直近の数回の会話だけを記録しておくことができるので、とても便利です。
実際には、Kが1に等しいときにこれを使うことはないでしょう。
Kをもっと大きな数値に設定して使うことになります。
しかし、それでも、会話が長くなるとメモリが際限なく増えていくのを防ぐことができます。
そこで、先ほどの会話を再実行するとしたら、「こんにちは、私の名前はアンドリューです」と言いましょう。
1プラス1ってなんですか？そして今度は、「私の名前は何ですか」と聞いてみます。
Kは1に等しいので、最後のやりとりしか覚えていないのですが、1＋1は何でしょうか？そう、1プラスイコール2なのです。
そして、この初期のやり取りを忘れてしまっていて、今は「すみません、その情報にはアクセスできません」となっています。
ひとつだけお願いしたいのは、ビデオを掲載することです。
左のコードでこれをtrueに変更し、verbose equals trueでこの会話を再実行します。
そうすると、実際にこのプロンプトを生成するために使用されたプロンプトを見ることができます。
そして、「私の名前は何ですか」というLMを呼び出したときに、メモリが「私の名前は何ですか」というやりとりを削除してしまったため、「私の名前は何ですか」と知らないことになっていることがおわかりいただけると思います。
会話トークンバッファメモリでは、メモリが保存するトークンの数を制限することになります。
LMの価格設定の多くはトークンに基づいているため、これは通話料金に直接的に対応することになります。
もし、トークンの上限を50とした場合、少しコメントさせてください。
例えば、AIはすごいね、バックプロパゲーション、美しいチャペル、魅力的だね、といった会話があったとします。
これらの会話の脅しの最初の文字としてABCを使っています。
いつ何が言われたかを記録しておくことができます。
そして、私はこれを高いトークンリミットのものを実行しています。
ほぼ全部の会話を収録しています。
トークン制限を100まで上げると、今度は全部の会話ができるようになりました。
AIのサインは何ですか？トークン制限を下げると、この会話の前の部分が切り落とされ、最新のやりとりに対応するトークン数が保持されますが、トークン制限を超えないことが条件となります。
なぜLOMを指定する必要があるのかというと、LOMによってトークンの数え方が異なるからです。
つまり、IOMを開いているチャットが使っているトークンの数え方を使うように指示するわけです。
ビデオを一時停止してコードを実行し、プロンプトを変更して別の出力が得られるかどうか試してみることをお勧めします。
最後に、会話の要約バッファメモリについて説明します。
このアイデアは、直近の他のインスタンスや決まった数の会話のやりとりに基づいてメモリを固定トークンの数に制限する代わりに、LOMを使ってこれまでの会話の要約を書き、それをメモリとすることです。
そこで、ある人のスケジュールで長い文字列を作る例を挙げます。
会議があり、製品チームがあり、パワーポイントのプレゼンが必要である、などなど。
つまり、これは「あなたのスケジュールは何ですか」という長い文字列なのです。
たぶん、最後はお客さんとイタリアンレストランでランチして、LOMの最新デモを見せるとか。
そこで、会話要約バッファ・メモリを使用します。
この場合、最大トークン数は400で、かなり高いトークン数です。
そして、いくつかの会話用語を挿入します。
まず、「こんにちは」、「どうしたの？それから、今日のスケジュールはどうなっていますか？という質問に対して、「長い予定です」と答えます。
このように、この記憶にはかなり多くのテキストが含まれています。
実際、メモリの変数を見てみましょう。
トークン1つ1つに対して、このテキストをすべて保存するのに十分な容量があったため、そのテキスト全体が入っているのです。
しかし、ここで、最大トークン数を100トークンに減らすとします。
これは会話履歴全体を保存するものです。
つまり、3日目のスケジュールとフォームの前に、人間とAIエンゲージメントのスモールトップがあり、朝のミーティング、ブラブラ、ランチミーティング、最新のAI開発におけるAIへの顧客の関心などです。
で、このLOMを使って会話をするとしたら、先ほどと同じように、会話チェーンを作ってみます。
そして、「どんなデモを見せたらいいでしょうか」とインプットするとします。
verboseはtrueと言ったので、ここにプロンプトが表示されます。
LOMは、今の会話はこれまでこのような議論をしてきたと考える、これが会話の要約だからだ。
そして、もしあなたがオープンAIチャットAPIエンドポイントに精通しているならば、この特定のシステムメッセージがあることに注意してください。
この例では、公式のオープンAIシステムメッセージを使用しているわけではなく、プロンプトの一部として含んでいるだけですが、それでもかなりうまく機能しています。
このプロンプトが表示されると、出力は基本的に、私たちの開発のコストに関わるもので、私たちの最新のNRP能力を紹介する提案になりますね。
なるほど、それはいい。
もし私があなたの顧客に会ったとしたら、LOMを使ったクールなNLPアプリケーションを構築するのに役立つオープンソースのフレームワークがあれば、こう言うだろうね。
良いものが欠けているのです。
興味深いのは、メモリに何が起こったかを見てみると、ここでは最新のAIシステムの出力が取り込まれていることに気づきます。
一方、私が「何を見せたらいいデモになるか」と尋ねた発言は、システムメッセージ、つまりこれまでの会話の全体概要に取り込まれています。
会話の要約をバッファメモリに保存することで、メッセージの明示的な保存を、制限として指定したトークン数まで維持しようとするのです。
つまり、明示的なストレージにもかかわらず、私たちは100トークンを上限としようとしています。
それ以上の場合は、CLMを使ってサマリーを生成します。
チャットを例にしてこれらのメモリを説明しましたが、これらのメモリは他のアプリケーションにも有効です。
しかし、新しいテキストの断片を取得し続けたり、新しい情報を取得し続けたりします。
例えば、システムが繰り返しオンラインで事実を検索する場合などです。
ビデオを一時停止して、コードを実行することをお勧めします。
このビデオでは、会話のやり取りやトークンの数に応じて制限するバッファ・メモリや、ある限度以上のトークンを要約できるメモリなど、いくつかのタイプのメモリが紹介されていましたね。
そのチェーンは、実は追加のメモリタイプもサポートしています。
中でも強力なのが、ベクターデータメモリです。
単語埋め込みやテキスト埋め込みに馴染みのある方なら、ベクトルデータベースは実際にそうした埋め込みを保存しています。
意味がわからなくても、ハリソンが後で説明するので心配しないでください。
そして、メモリとしてのベクトルデータベースの種類を使って、最も関連性の高いテキストブロックを取り出すことができる。
例えば、特定の友人について話すと、Lanchineはその友人についての事実を明示的に記憶することができるのです。
Lanchineを使ったアプリケーションの実装では、複数の種類の記憶を使うこともできます。
例えば、このビデオで見たような会話記憶と、さらに個人を呼び出すことによる実体記憶を使うことができます。
このように、会話の要約を記憶したり、会話に登場する重要な人物の重要な事実を明示的に記憶したりすることができます。
もちろん、これらのメモリ・タイプを使用するだけでなく、開発者は会話全体を従来のデータベース（キーバリューストアやSQLデータベース）に保存することも珍しくありません。
以上、メモリタイプについて説明しました。
自分のアプリケーションを構築する際に、この方法が役に立つことを願っています。
それでは次のビデオで、Lanchineの重要な構成要素であるチェーンについて学びましょう。
